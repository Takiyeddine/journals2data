"""
This type stub file was generated by pyright.
"""

"""
Newspaper uses much of python-goose's extraction code. View their license:
https://github.com/codelucas/newspaper/blob/master/GOOSE-LICENSE.txt

Keep all html page extraction code within this file. Abstract any
lxml or soup parsing code in the parsers.py file!
"""
__title__ = ...
__author__ = ...
__license__ = ...
__copyright__ = ...
log = ...
MOTLEY_REPLACEMENT = ...
ESCAPED_FRAGMENT_REPLACEMENT = ...
TITLE_REPLACEMENTS = ...
PIPE_SPLITTER = ...
DASH_SPLITTER = ...
UNDERSCORE_SPLITTER = ...
SLASH_SPLITTER = ...
ARROWS_SPLITTER = ...
COLON_SPLITTER = ...
SPACE_SPLITTER = ...
NO_STRINGS = ...
A_REL_TAG_SELECTOR = ...
A_HREF_TAG_SELECTOR = ...
RE_LANG = ...
good_paths = ...
bad_chunks = ...
bad_domains = ...
class ContentExtractor:
    def __init__(self, config) -> None:
        ...
    
    def update_language(self, meta_lang): # -> None:
        """Required to be called before the extraction process in some
        cases because the stopwords_class has to set incase the lang
        is not latin based
        """
        ...
    
    def get_authors(self, doc): # -> list[Unknown]:
        """Fetch the authors of the article, return as a list
        Only works for english articles
        """
        ...
    
    def get_publishing_date(self, url, doc): # -> datetime | None:
        """3 strategies for publishing date extraction. The strategies
        are descending in accuracy and the next strategy is only
        attempted if a preferred one fails.

        1. Pubdate from URL
        2. Pubdate from metadata
        3. Raw regex searches in the HTML + added heuristics
        """
        ...
    
    def get_title(self, doc): # -> str:
        """Fetch the article title and analyze it

        Assumptions:
        - title tag is the most reliable (inherited from Goose)
        - h1, if properly detected, is the best (visible to users)
        - og:title and h1 can help improve the title extraction
        - python == is too strict, often we need to compare filtered
          versions, i.e. lowercase and ignoring special chars

        Explicit rules:
        1. title == h1, no need to split
        2. h1 similar to og:title, use h1
        3. title contains h1, title contains og:title, len(h1) > len(og:title), use h1
        4. title starts with og:title, use og:title
        5. use title, after splitting
        """
        ...
    
    def split_title(self, title, splitter, hint=...): # -> str:
        """Split the title to best part possible
        """
        ...
    
    def get_feed_urls(self, source_url, categories): # -> list[Unknown | str]:
        """Takes a source url and a list of category objects and returns
        a list of feed urls
        """
        ...
    
    def get_favicon(self, doc): # -> Literal['']:
        """Extract the favicon from a website http://en.wikipedia.org/wiki/Favicon
        <link rel="shortcut icon" type="image/png" href="favicon.png" />
        <link rel="icon" type="image/png" href="favicon.png" />
        """
        ...
    
    def get_meta_lang(self, doc): # -> None:
        """Extract content language from meta
        """
        ...
    
    def get_meta_content(self, doc, metaname): # -> Literal['']:
        """Extract a given meta content form document.
        Example metaNames:
            "meta[name=description]"
            "meta[name=keywords]"
            "meta[property=og:type]"
        """
        ...
    
    def get_meta_img_url(self, article_url, doc): # -> Literal['']:
        """Returns the 'top img' as specified by the website
        """
        ...
    
    def get_meta_type(self, doc): # -> Literal['']:
        """Returns meta type of article, open graph protocol
        """
        ...
    
    def get_meta_description(self, doc): # -> Literal['']:
        """If the article has meta description set in the source, use that
        """
        ...
    
    def get_meta_keywords(self, doc): # -> Literal['']:
        """If the article has meta keywords set in the source, use that
        """
        ...
    
    def get_meta_data(self, doc): # -> defaultdict[Unknown, dict[Unknown, Unknown]]:
        ...
    
    def get_canonical_link(self, article_url, doc): # -> str:
        """
        Return the article's canonical URL

        Gets the first available value of:
        1. The rel=canonical tag
        2. The og:url tag
        """
        ...
    
    def get_img_urls(self, article_url, doc): # -> set[Unknown]:
        """Return all of the images on an html page, lxml root
        """
        ...
    
    def get_first_img_url(self, article_url, top_node): # -> Literal['']:
        """Retrieves the first image in the 'top_node'
        The top node is essentially the HTML markdown where the main
        article lies and the first image in that area is probably signifigcant.
        """
        ...
    
    def get_urls(self, doc_or_html, titles=..., regex=...): # -> list[Any] | list[tuple[Unknown, Unknown]] | list[Unknown]:
        """`doc_or_html`s html page or doc and returns list of urls, the regex
        flag indicates we don't parse via lxml and just search the html.
        """
        ...
    
    def get_category_urls(self, source_url, doc): # -> list[Unknown | str]:
        """Inputs source lxml root and source url, extracts domain and
        finds all of the top level urls, we are assuming that these are
        the category urls.
        cnn.com --> [cnn.com/latest, world.cnn.com, cnn.com/asia]
        """
        ...
    
    def extract_tags(self, doc): # -> set[Unknown]:
        ...
    
    def calculate_best_node(self, doc): # -> None:
        ...
    
    def is_boostable(self, node): # -> bool:
        """A lot of times the first paragraph might be the caption under an image
        so we'll want to make sure if we're going to boost a parent node that
        it should be connected to other paragraphs, at least for the first n
        paragraphs so we'll want to make sure that the next sibling is a
        paragraph and has at least some substantial weight to it.
        """
        ...
    
    def walk_siblings(self, node):
        ...
    
    def add_siblings(self, top_node):
        ...
    
    def get_siblings_content(self, current_sibling, baseline_score_siblings_para): # -> list[Unknown] | None:
        """Adds any siblings that may have a decent score to this node
        """
        ...
    
    def get_siblings_score(self, top_node): # -> float | Literal[100000]:
        """We could have long articles that have tons of paragraphs
        so if we tried to calculate the base score against
        the total text score of those paragraphs it would be unfair.
        So we need to normalize the score based on the average scoring
        of the paragraphs within the top node.
        For example if our total score of 10 paragraphs was 1000
        but each had an average value of 100 then 100 should be our base.
        """
        ...
    
    def update_score(self, node, add_to_score): # -> None:
        """Adds a score to the gravityScore Attribute we put on divs
        we'll get the current score then add the score we're passing
        in to the current.
        """
        ...
    
    def update_node_count(self, node, add_to_count): # -> None:
        """Stores how many decent nodes are under a parent node
        """
        ...
    
    def is_highlink_density(self, e): # -> bool:
        """Checks the density of links within a node, if there is a high
        link to text ratio, then the text is less likely to be relevant
        """
        ...
    
    def get_score(self, node): # -> float | Literal[0]:
        """Returns the gravityScore as an integer from this node
        """
        ...
    
    def get_node_gravity_score(self, node): # -> float | None:
        ...
    
    def nodes_to_check(self, doc): # -> list[Unknown]:
        """Returns a list of nodes we want to search
        on like paragraphs and tables
        """
        ...
    
    def is_table_and_no_para_exist(self, e): # -> bool:
        ...
    
    def is_nodescore_threshold_met(self, node, e): # -> bool:
        ...
    
    def post_cleanup(self, top_node):
        """Remove any divs that looks like non-content, clusters of links,
        or paras with no gusto; add adjacent nodes which look contenty
        """
        ...
    


