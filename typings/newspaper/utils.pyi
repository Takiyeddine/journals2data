"""
This type stub file was generated by pyright.
"""

"""
Holds misc. utility methods which prove to be
useful throughout this library.
"""
__title__ = ...
__author__ = ...
__license__ = ...
__copyright__ = ...
log = ...
class FileHelper:
    @staticmethod
    def loadResourceFile(filename): # -> str:
        ...
    


class ParsingCandidate:
    def __init__(self, url, link_hash) -> None:
        ...
    


class RawHelper:
    @staticmethod
    def get_parsing_candidate(url, raw_html): # -> ParsingCandidate:
        ...
    


class URLHelper:
    @staticmethod
    def get_parsing_candidate(url_to_crawl): # -> ParsingCandidate:
        ...
    


class StringSplitter:
    def __init__(self, pattern) -> None:
        ...
    
    def split(self, string): # -> list[Unknown]:
        ...
    


class StringReplacement:
    def __init__(self, pattern, replaceWith) -> None:
        ...
    
    def replaceAll(self, string): # -> Literal['']:
        ...
    


class ReplaceSequence:
    def __init__(self) -> None:
        ...
    
    def create(self, firstPattern, replaceWith=...): # -> ReplaceSequence:
        ...
    
    def append(self, pattern, replaceWith=...): # -> ReplaceSequence:
        ...
    
    def replaceAll(self, string): # -> Literal['']:
        ...
    


class TimeoutError(Exception):
    ...


def timelimit(timeout): # -> (function: Unknown) -> (*args: Unknown, **kw: Unknown) -> Unknown | None:
    """Borrowed from web.py, rip Aaron Swartz
    """
    ...

def domain_to_filename(domain):
    """All '/' are turned into '-', no trailing. schema's
    are gone, only the raw domain + ".txt" remains
    """
    ...

def filename_to_domain(filename):
    """[:-4] for the .txt at end
    """
    ...

def is_ascii(word): # -> bool:
    """True if a word is only ascii chars
    """
    ...

def extract_meta_refresh(html): # -> None:
    """ Parses html for a tag like:
    <meta http-equiv="refresh" content="0;URL='http://sfbay.craigslist.org/eby/cto/5617800926.html'" />
    Example can be found at: https://www.google.com/url?rct=j&sa=t&url=http://sfbay.craigslist.org/eby/cto/
    5617800926.html&ct=ga&cd=CAAYATIaYTc4ZTgzYjAwOTAwY2M4Yjpjb206ZW46VVM&usg=AFQjCNF7zAl6JPuEsV4PbEzBomJTUpX4Lg
    """
    ...

def to_valid_filename(s): # -> str:
    """Converts arbitrary string (for us domain name)
    into a valid file name for caching
    """
    ...

def cache_disk(seconds=..., cache_folder=...): # -> (function: Unknown) -> (*args: Unknown, **kwargs: Unknown) -> (Any | Unknown):
    """Caching extracting category locations & rss feeds for 5 days
    """
    ...

def print_duration(method): # -> (*args: Unknown, **kw: Unknown) -> Unknown:
    """Prints out the runtime duration of a method in seconds
    """
    ...

def chunks(l, n): # -> Generator[Unknown, None, None]:
    """Yield n successive chunks from l
    """
    ...

def purge(fn, pattern): # -> None:
    """Delete files in a dir matching pattern
    """
    ...

def clear_memo_cache(source): # -> None:
    """Clears the memoization cache for this specific news domain
    """
    ...

def memoize_articles(source, articles): # -> list[Unknown]:
    """When we parse the <a> links in an <html> page, on the 2nd run
    and later, check the <a> links of previous runs. If they match,
    it means the link must not be an article, because article urls
    change as time passes. This method also uniquifies articles.
    """
    ...

def get_useragent(): # -> str:
    """Uses generator to return next useragent in saved file
    """
    ...

def get_available_languages(): # -> list[str]:
    """Returns a list of available languages and their 2 char input codes
    """
    ...

def print_available_languages(): # -> None:
    """Prints available languages with their full names
    """
    ...

def extend_config(config, config_items):
    """
    We are handling config value setting like this for a cleaner api.
    Users just need to pass in a named param to this source and we can
    dynamically generate a config object for it.
    """
    ...

